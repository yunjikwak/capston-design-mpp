{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# For static images:\n",
    "IMAGE_FILES = []\n",
    "BG_COLOR = (192, 192, 192) # gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_pose.Pose(\n",
    "    static_image_mode=True,\n",
    "    model_complexity=2,\n",
    "    enable_segmentation=True,\n",
    "    min_detection_confidence=0.5) as pose:\n",
    "  for idx, file in enumerate(IMAGE_FILES):\n",
    "    image = cv2.imread(file)\n",
    "    image_height, image_width, _ = image.shape\n",
    "    # Convert the BGR image to RGB before processing.\n",
    "    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    if not results.pose_landmarks:\n",
    "      continue\n",
    "    print(\n",
    "        f'Nose coordinates: ('\n",
    "        f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE].x * image_width}, '\n",
    "        f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE].y * image_height})'\n",
    "    )\n",
    "\n",
    "    annotated_image = image.copy()\n",
    "    # Draw segmentation on the image.\n",
    "    # To improve segmentation around boundaries, consider applying a joint\n",
    "    # bilateral filter to \"results.segmentation_mask\" with \"image\".\n",
    "    condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.1\n",
    "    bg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "    bg_image[:] = BG_COLOR\n",
    "    annotated_image = np.where(condition, annotated_image, bg_image)\n",
    "    # Draw pose landmarks on the image.\n",
    "    mp_drawing.draw_landmarks(\n",
    "        annotated_image,\n",
    "        results.pose_landmarks,\n",
    "        mp_pose.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "    cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', annotated_image)\n",
    "    # Plot pose world landmarks.\n",
    "    mp_drawing.plot_landmarks(\n",
    "        results.pose_world_landmarks, mp_pose.POSE_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1회 쓰러짐 감지\n",
      "2회 쓰러짐 감지\n",
      "3회 쓰러짐 감지\n",
      "4회 쓰러짐 감지\n",
      "5회 쓰러짐 감지\n",
      "6회 쓰러짐 감지\n",
      "7회 쓰러짐 감지\n",
      "8회 쓰러짐 감지\n",
      "9회 쓰러짐 감지\n",
      "10회 쓰러짐 감지\n",
      "11회 쓰러짐 감지\n",
      "12회 쓰러짐 감지\n",
      "13회 쓰러짐 감지\n",
      "14회 쓰러짐 감지\n",
      "15회 쓰러짐 감지\n",
      "16회 쓰러짐 감지\n",
      "17회 쓰러짐 감지\n",
      "18회 쓰러짐 감지\n",
      "19회 쓰러짐 감지\n",
      "20회 쓰러짐 감지\n",
      "21회 쓰러짐 감지\n",
      "22회 쓰러짐 감지\n",
      "23회 쓰러짐 감지\n",
      "24회 쓰러짐 감지\n",
      "25회 쓰러짐 감지\n",
      "26회 쓰러짐 감지\n",
      "27회 쓰러짐 감지\n",
      "28회 쓰러짐 감지\n",
      "29회 쓰러짐 감지\n",
      "30회 쓰러짐 감지\n",
      "31회 쓰러짐 감지\n",
      "32회 쓰러짐 감지\n",
      "33회 쓰러짐 감지\n",
      "34회 쓰러짐 감지\n",
      "35회 쓰러짐 감지\n",
      "36회 쓰러짐 감지\n",
      "37회 쓰러짐 감지\n",
      "38회 쓰러짐 감지\n",
      "39회 쓰러짐 감지\n",
      "40회 쓰러짐 감지\n",
      "41회 쓰러짐 감지\n",
      "42회 쓰러짐 감지\n",
      "43회 쓰러짐 감지\n",
      "44회 쓰러짐 감지\n",
      "45회 쓰러짐 감지\n",
      "46회 쓰러짐 감지\n",
      "47회 쓰러짐 감지\n",
      "48회 쓰러짐 감지\n",
      "49회 쓰러짐 감지\n",
      "50회 쓰러짐 감지\n",
      "51회 쓰러짐 감지\n",
      "52회 쓰러짐 감지\n",
      "53회 쓰러짐 감지\n",
      "54회 쓰러짐 감지\n",
      "55회 쓰러짐 감지\n",
      "56회 쓰러짐 감지\n",
      "57회 쓰러짐 감지\n",
      "58회 쓰러짐 감지\n",
      "59회 쓰러짐 감지\n",
      "60회 쓰러짐 감지\n",
      "61회 쓰러짐 감지\n",
      "62회 쓰러짐 감지\n",
      "63회 쓰러짐 감지\n",
      "64회 쓰러짐 감지\n",
      "65회 쓰러짐 감지\n",
      "66회 쓰러짐 감지\n",
      "67회 쓰러짐 감지\n",
      "68회 쓰러짐 감지\n",
      "69회 쓰러짐 감지\n",
      "70회 쓰러짐 감지\n",
      "71회 쓰러짐 감지\n",
      "72회 쓰러짐 감지\n",
      "73회 쓰러짐 감지\n",
      "74회 쓰러짐 감지\n",
      "75회 쓰러짐 감지\n",
      "76회 쓰러짐 감지\n",
      "77회 쓰러짐 감지\n",
      "78회 쓰러짐 감지\n",
      "79회 쓰러짐 감지\n",
      "80회 쓰러짐 감지\n",
      "81회 쓰러짐 감지\n",
      "82회 쓰러짐 감지\n",
      "83회 쓰러짐 감지\n",
      "84회 쓰러짐 감지\n",
      "85회 쓰러짐 감지\n",
      "86회 쓰러짐 감지\n",
      "87회 쓰러짐 감지\n",
      "88회 쓰러짐 감지\n",
      "89회 쓰러짐 감지\n",
      "90회 쓰러짐 감지\n",
      "91회 쓰러짐 감지\n",
      "92회 쓰러짐 감지\n",
      "93회 쓰러짐 감지\n",
      "Ignoring empty camera frame.\n"
     ]
    }
   ],
   "source": [
    "# For webcam input:\n",
    "# cap = cv2.VideoCapture(0)\n",
    "video_path = 'C:\\\\Users\\\\mia00\\\\Desktop\\\\CD\\\\resources\\\\10240439-uhd_4096_2160_25fps.mp4' # 비디오 파일 경로\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "pTime = 0 # fps 계산 위한 previous 타임\n",
    "cnt = 0 # fps 계산 위한 카운트\n",
    "\n",
    "with mp_pose.Pose(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as pose:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      # continue\n",
    "      break\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = pose.process(image)\n",
    "\n",
    "    keypoints = []\n",
    "    if results.pose_landmarks:\n",
    "        for data_point in results.pose_landmarks.landmark:\n",
    "            keypoints.append({\n",
    "                'X': data_point.x,\n",
    "                'Y': data_point.y,\n",
    "                'Z': data_point.z,\n",
    "                'Visibility': data_point.visibility,\n",
    "            })\n",
    "\n",
    "        # 쓰러짐 감지 예제\n",
    "        a = keypoints[10]['Y']\n",
    "        b = keypoints[24]['Y']\n",
    "        diff = abs(b-a)\n",
    "        if diff < 0.1 :\n",
    "          cnt += 1\n",
    "          print(f'{cnt}회 쓰러짐 감지')\n",
    "        # request = requests.post('http:// ~~ 서버에 보내기\n",
    "\n",
    "    # fps\n",
    "    cTime = time.time() # 현재 시간\n",
    "    fps = 1 / (cTime - pTime)\n",
    "    pTime = cTime\n",
    "\n",
    "    cv2.putText(image, f'FPS: {int(fps)}', (40, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 3)\n",
    "\n",
    "    # Draw the pose annotation on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_pose.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "\n",
    "    resized_image = cv2.resize(image, (960, 540))\n",
    "\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    # cv2.imshow('MediaPipe Pose', cv2.flip(image, 1))\n",
    "    cv2.imshow('MediaPipe Pose', resized_image) # 반전 안함\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
